---
title: "Principal Component Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Principal Component Analysis}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

# Principal Component Analysis {.unnumbered}

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(PCAmethod)
```

## Overview {.unnumbered}

This vignette provides an overview of how to conduct a principal component 
analysis (PCA). PCA is a statistical method used to compress high-dimensional 
(>3) data and retain the most informative aspects. This is done by transforming 
the original data into a lower-dimensional space while collating highly 
correlated variables together.

## Introduction {.unnumbered}

Principal component analysis is a way to analyze and draw conclusions from more than three variables. For example, we could ask what contributes to graduate student well-being from a data set that includes age, research progress, financial stability, personal growth, health, and recognition/validation. To draw conclusions about graduate student well-being, we first need to visualize and analyze this data set. Visualizing in more than three dimensions is difficult, thus visualizing graduate student well-being data with six variables (dimensions) cannot be done efficiently. PCAs take all factors (variables), combines them in a smart way, and produces new factors that are (1) correlated with each other and (2) ranked from most important to least important. The new factors produced by PCA are called **principal components**.  

#### How does a PCA pick its components? 

PCA asks the question "How can we arrange these data to preserve as much information as possible?" This results in an optimization problem using Lagrange Multipliers that collates the most contributory variables in order to preserve as much information as possible.

#### How is a PCA conducted? 

1. Data normalization: Each variable must be *quantitative*. Data will be normalized by subtracting its mean and dividing by its standard deviation. 
2. Covariance matrix: Compute a symmetric matrix generated from the covariance, a measure of the total variation of two random variables from their expected values, between all pairs of variables. 
3. Eigenvectors and eigenvalues: Eigenvector represents a direction such as "vertical" or "90 degrees". An eigenvalue, on the other hand, is a number representing the amount of variance present in the data for a given direction. Each eigenvector has its corresponding eigenvalue. 
4. Selection of principal components. There are as many pairs of eigenvectors and eigenvalues as the number of variables in the data. In the data with age, research progress, financial stability, personal growth, health, and recognition and validation, there will be fifteen pairs; **not** all the pairs are relevant. So, the eigenvector with the highest eigenvalue corresponds to the first principal component. The second principal component is the eigenvector with the second highest eigenvalue, and so on.
5.  Data transformation in new dimensional space: this step involves re-orienting the original data onto a new subspace defined by the principal components This reorientation is done by multiplying the original data by the previously computed eigenvectors.

#### Applications for PCA 
Principal component analysis has a variety of applications in our day-to-day life, including finance, image processing, healthcare, and security. In our personal research, PCAs are commonly used to better analyze/understand population clustering as well as genetic and behavioral analysis. 

This vignette was based on the following sources

* [Principal Component Analysis in R Tutorial](https://www.datacamp.com/tutorial/pca-analysis-r)
* [StatQuest: Principal Component Analysis(PCA), Step-by-Step](https://www.youtube.com/watch?v=FgakZw6K1QQ)

## Usage {.unnumbered}

Here, we created a package for Principal Component Analysis (PCA) using existing functions.

The following code uses the **music** dataset bundled in the {PCAmethod} package to run a principal component analysis of several variables that contribute to music genre. We can use PCA to determine which variables combine to explain the greatest amount of the variance in the data set. 

### Preliminaries

The **music** dataset is derived from the [music-genre-classification train.csv](https://www.kaggle.com/datasets/purumalgi/music-genre-classification/data?select=train.csv) dataset in ***Kaggle*** and was created during a **MachineHack Hackathon**. The dataset includes 17,996 songs with 17 metrics (artist name; track name; popularity; ‘danceability’; energy; key; loudness; mode; ‘speechiness’; ‘acousticness’; ‘instrumentalness’; liveness; valence; tempo; duration in milliseconds and time_signature). **Class** is the target variable (genre) ranging from 0-11 and indicating Rock, Indie, Alt, Pop, Metal, HipHop, Alt_Music, Blues, Acoustic/Folk, Instrumental, Country, or Bollywood.

Package dependencies
```{r, warning = FALSE}
library(tidyverse)
library(ggplot2)
```

Check the data set and determine which variables are qualitative and which are quantitative.
```{r}
head(music)
```

### Step 1: Clean the data

PCA only works with numerical values. So, we need to remove **Artist Name** and **Track Name** columns. Also, the **Class** column is not relevant to the analysis since it is refers to the genres themselves.

The code below creates a new data frame with only numeric columns. It also removes rows (songs) containing NA values

```{r}
music_clean <- music |> select(-"Artist Name", -"Track Name", -"Class") |> drop_na()

# select() from dplyr, drop_na() from tidyr
```

### Step 2: Use `princomp` function to run PCA

`princomp()` performs a principal components analysis on the given numeric data matrix, passed in as argument x, and returns the results as an object of class princomp. 

This function is from the {stats} package in base R.

R Core Team (2022). R: A language and environment for statistical computing. R Foundation
  for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.

```{r}
pc <- princomp(x = music_clean)
```

### Step 3: Use `autoplot_pca` function to visualize PCA

`autoplot_pca()` takes the following arguments [1] an `object` returned by a function performing PCA, [2] a `mapping` call to `aes()` specifying additional mappings between variables and plot aesthetics, [3] `data` the data frame associated with the PCA, and returns a ggplot2 object.

<cite source>

```{r, eval = FALSE}
ggbiplot(pc)
# need to edit autoplot_pca.R file to call `ggplot2::`
```


How much of the variance in the data set is explained by PC1? By PC2? Which variables contribute to PC1 (and explaining the greatest amount of variance)? 

## Add a Data Directory and Data

The **music** dataset is used for example purposes. It is derived from the [music-genre-classification train.csv](https://www.kaggle.com/datasets/purumalgi/music-genre-classification/data?select=train.csv) dataset in ***Kaggle*** and was created during a **MachineHack Hackathon**. The dataset includes 17,996 songs with 17 metrics (artist name; track name; popularity; ‘danceability’; energy; key; loudness; mode; ‘speechiness’; ‘acousticness’; ‘instrumentalness’; liveness; valence; tempo; duration in milliseconds and time_signature). **Class** is the target variable (genre) ranging from 0-11 and indicating Rock, Indie, Alt, Pop, Metal, HipHop, Alt_Music, Blues, Acoustic/Folk, Instrumental, Country, or Bollywood.

```{r, eval = FALSE}
library(tidyverse)
f <- "data/music.csv"
music <- read_csv(f, col_names = TRUE)
usethis::use_data(music, overwrite = TRUE)
# overwrite argument replaces the data if it already exists
```
